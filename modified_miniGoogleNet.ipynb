{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modified_miniGoogleNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "31GkNutZILKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "\n",
        "def factorize(n):\n",
        "  # факторизация на множители >= 2\n",
        "  fact = []\n",
        "  d = 2\n",
        "  while (d * d <= n):\n",
        "    if n % d == 0:\n",
        "      fact.append(d)\n",
        "      n = n // d\n",
        "    else:\n",
        "      d += 1\n",
        "  if n > 1:\n",
        "    fact.append(n)\n",
        "  return sorted(fact)\n",
        "\n",
        "def toNdim(a, n):\n",
        "    while len(a) > n:\n",
        "      c = copy.copy(a)\n",
        "      a = sorted([c[0]*c[1]] + c[2::])\n",
        "    return a\n",
        "\n",
        "def vector_equal(a, b, n):\n",
        "  # выравнивание длин векторов до заданной длины\n",
        "  if len(a) > n:\n",
        "    a = toNdim(a, n)\n",
        "  if len(b) > n:\n",
        "    b = toNdim(b, n)\n",
        "  if len(a) < n:\n",
        "    a = [1]* (n - len(a)) + a\n",
        "  if len(b) < n:\n",
        "    b = [1]* (n - len(b)) + b\n",
        "  return a, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjDgg9OOVKbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.keras.initializers import *\n",
        "from tensorflow.keras.backend import placeholder\n",
        "from tensorflow.keras.layers import *\n",
        "import random\n",
        "\n",
        "\n",
        "def ttconv(inp, out_ch, d, window=(1, 1), strides=[1, 1], padding = 'SAME',\n",
        "           initializers = GlorotUniform(), regularizers = None):\n",
        "  ''' Tensor Train decomposition for convolution\n",
        "      Источник: https:// github.com/ timgaripov/TensorNet-TF\n",
        "      inp: input - [batch_size, width, height, in_chan]\n",
        "      out_ch: number of output channels\n",
        "      window: convolution window\n",
        "\n",
        "\n",
        "      in_imag      - input image\n",
        "      in_ch_dims   - factorization for dimension of input channel,   i = 0, ..., d-1\n",
        "      out_ch_dims  -                                output\n",
        "      d            - number of TT kernels\n",
        "      ranks        - ranks for TT kernels\n",
        "      ranks[0] = ranks[d] = 1\n",
        "  '''\n",
        "  if padding == 'same':\n",
        "    padding = 'SAME'\n",
        "  else:\n",
        "    padding = 'VALID'\n",
        "\n",
        "  in_h, in_w, in_ch = inp.get_shape().as_list()[1:]\n",
        "  in_ch_dims, out_ch_dims = vector_equal(factorize(in_ch), factorize(out_ch), n)\n",
        "  in_imag = tf.reshape(inp, [-1, in_h, in_w, in_ch])\n",
        "\n",
        "  ranks = [0] * (d + 1)\n",
        "  for i in range(0, d):\n",
        "    ranks[i] = random.randint(2, 4)\n",
        "  ranks[d] = 1\n",
        "\n",
        "  filter_shape = [window[0], window[1], 1, ranks[0]] # first kernel\n",
        "  if window[0]*window[1]*1*ranks[0] == 1:\n",
        "    filters = tf.compat.v1.get_variable('filters',shape=filter_shape,\n",
        "                                        initializer=Ones(),\n",
        "                                        regularizer=regularizers)\n",
        "  else:\n",
        "    filters = tf.compat.v1.get_variable('filters',shape=filter_shape,\n",
        "                                        initializer=initializers,\n",
        "                                        regularizer=regularizers)\n",
        "  kernels = []\n",
        "  for i in range(d):\n",
        "    kernels.append(initializers(shape=[out_ch_dims[i] * ranks[i + 1], ranks[i] * in_ch_dims[i]])) \n",
        "\n",
        "  conv = filters\n",
        "  for i in range(d):            \n",
        "    conv = tf.reshape(conv, [-1, ranks[i]])\n",
        "    kernel = tf.transpose(kernels[i], [1, 0])\n",
        "    kernel = tf.reshape(kernel, [ranks[i], -1])\n",
        "    conv = tf.matmul(conv, kernel)\n",
        "\n",
        "  conv_shape = [window[0], window[1]]\n",
        "  order, in_order, out_order = [0, 1], [], []\n",
        "  for i in range(d):\n",
        "      conv_shape.append(in_ch_dims[i])\n",
        "      in_order.append(2 + 2 * i)\n",
        "      conv_shape.append(out_ch_dims[i])\n",
        "      out_order.append(2 + 2 * i + 1)\n",
        "  order += in_order + out_order\n",
        "  conv = tf.reshape(conv, conv_shape)\n",
        "  conv = tf.transpose(conv, order)\n",
        "  conv = tf.reshape(conv, [window[0], window[1], in_ch, out_ch])\n",
        "\n",
        "  return tf.nn.conv2d(in_imag, conv, [1] + strides + [1], padding=padding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zgnu5E-0qgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.regularizers import l2, l1\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from numpy.testing import assert_allclose"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA0sGKS604jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Для работы в Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilF5G6Fa8bpI",
        "colab_type": "text"
      },
      "source": [
        "## miniGoogleNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnhKDBGX5cgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from keras import backend as K\n",
        "\n",
        "def miniGoogleNet(width, height, depth, classes):\n",
        "    def conv_module(x, K, kX, kY, A, B, stride, chanDim, padding=\"same\"):\n",
        "        # define convolution -> batch normalization -> ReLU\n",
        "        if A <= 1 or B <= 1 or kX*kY == 1:\n",
        "          # standart convolutional\n",
        "          x = Conv2D(K, (kX, kY), strides=(stride[0], stride[1]), padding=padding)(x)\n",
        "        else:\n",
        "          # Tucker-2 +TT decomposition\n",
        "          x = Conv2D(A, (1, 1), strides=(1, 1), padding=padding)(x)\n",
        "          x = Conv2D(B, (kX, kY), strides=(1, 1), padding=padding)(x)\n",
        "          x = ttconv(x, K, 2, window=(1, 1), strides=stride, padding=padding, regularizers=l2(0.01))\n",
        "        x = BatchNormalization(axis=chanDim)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        return x\n",
        "\n",
        "    def inception_module(x, numK1x1, numK3x3, A, B, chanDim):\n",
        "        # define two conv_modules, then concatenate them\n",
        "        # across the channel dimension\n",
        "        conv_1x1 = conv_module(x, numK1x1, 1, 1, -1, -1, [1, 1], chanDim)\n",
        "        conv_3x3 = conv_module(x, numK3x3, 3, 3, A, B, [1, 1], chanDim)\n",
        "        x = concatenate([conv_1x1, conv_3x3], axis=chanDim)\n",
        "        return x\n",
        "\n",
        "    def downsample_module(x, K, A, B, chanDim):\n",
        "        # define the conv_module and pooling, then concatenate them\n",
        "        # across the channel dimensions\n",
        "        conv_3x3 = conv_module(x, K, 3, 3, A, B, [2, 2], chanDim, padding=\"valid\")\n",
        "        pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "        x = concatenate([conv_3x3, pool], axis=chanDim)\n",
        "        return x\n",
        "\n",
        "    # initialize the input shape to be \"channels last\" and the\n",
        "    # channels dimension itself\n",
        "    inputShape = (height, width, depth)\n",
        "    chanDim = -1\n",
        "\n",
        "    # if we are using \"channels first\", update the input shape\n",
        "    # and channels dimension\n",
        "    if K.image_data_format() == \"channels_first\":\n",
        "        inputShape = (depth, height, width)\n",
        "        chanDim = 1\n",
        "\n",
        "    # define the model input and first CONV module\n",
        "    inputs = Input(shape=inputShape)\n",
        "    x = conv_module(inputs, 96, 3, 3, A, B, [1, 1], chanDim)\n",
        "\n",
        "    # two Inception modules followed by a downsample module\n",
        "    x = inception_module(x, 32, 32, A, B, chanDim)\n",
        "    x = inception_module(x, 32, 48, A, B, chanDim)\n",
        "    x = downsample_module(x, 80, A, B, chanDim)\n",
        "    \n",
        "    # four Inception modules followed by a downsample module\n",
        "    x = inception_module(x, 112, 48, A, B, chanDim)\n",
        "    x = inception_module(x, 96, 64, A, B, chanDim)\n",
        "    x = inception_module(x, 80, 80, A, B, chanDim)\n",
        "    x = inception_module(x, 48, 96, A, B, chanDim)\n",
        "    x = downsample_module(x, 96, A, B, chanDim)\n",
        "\n",
        "    # two Inception modules followed by global POOL and dropout\n",
        "    x = inception_module(x, 176, 160, A, B, chanDim)\n",
        "    x = inception_module(x, 176, 160, A, B, chanDim)\n",
        "    x = AveragePooling2D((7, 7))(x)\n",
        "    x = BatchNormalization(axis=chanDim)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    \n",
        "\n",
        "    # softmax classifier\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(classes)(x)\n",
        "    x = Activation(\"softmax\")(x)\n",
        "\n",
        "    # create the model\n",
        "    model = Model(inputs, x, name=\"miniGoogleNet\")\n",
        "    # return the constructed network architecture\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCAb7dvzxlsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    lr = 0.01\n",
        "    if epoch > 90:\n",
        "        lr = 0.0001\n",
        "    elif epoch > 50:\n",
        "        lr = 0.001\n",
        "    elif epoch > 40:\n",
        "        lr = 0.01\n",
        "    print(\"lr = \", lr)\n",
        "    return lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEp6Z9Tk8e0h",
        "colab_type": "text"
      },
      "source": [
        "## init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9RFj3C-66oy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f19246ab-d06d-49d6-8cd0-4d8fba58087b"
      },
      "source": [
        "# initialize the initial learning rate, batch size, and number of\n",
        "# epochs to train for\n",
        "INIT_LR = lr_schedule(0)\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 100\n",
        "A = 8\n",
        "B = 8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr =  0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brfdEn3-6-GG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize the label names for the CIFAR-10 dataset\n",
        "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\",\n",
        "\t\"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "# load the CIFAR-10 dataset\n",
        "print(\"[INFO] loading CIFAR-10 dataset...\")\n",
        "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
        "\n",
        "# scale the data to the range [0, 1]\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.transform(testY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDKdpKRK7Ota",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# construct the image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=18, zoom_range=0.15,\n",
        "\twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "\t horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "print('Loading model...')\n",
        "model = miniGoogleNet(32, 32, 3, len(labelNames))\n",
        "\n",
        "# initialize the optimizer and compile the model\n",
        "opt = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS)\n",
        "print(\"[INFO] training network...\")\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g2xu-bG7W5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath = \"/content/drive/My Drive/Colab/checkpoint_new.h5/\"\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "# define the checkpoint\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', mode='min', save_best_only=True, verbose=1)\n",
        "callbacks_list =  [checkpoint, lr_scheduler]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG9mr8zS73aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "# train the network\n",
        "H = model.fit_generator(\n",
        "  aug.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
        "  validation_data=(testX, testY),\n",
        "  steps_per_epoch=trainX.shape[0] // BATCH_SIZE,\n",
        "  epochs=NUM_EPOCHS,\n",
        "  verbose=1,\n",
        "  callbacks=callbacks_list)\n",
        "\n",
        "duration = time.time() - start\n",
        "print(\"{} s to get output\".format(duration))\n",
        "\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/Colab/new2tt', 'wb') as file_pi:\n",
        "        pickle.dump(H.history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnbaWOi477os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the network\n",
        "import time\n",
        "print(\"[INFO] evaluating prediction...\")\n",
        "start = time.time()\n",
        "predictions = model.predict(testX, batch_size=BATCH_SIZE)\n",
        "duration = time.time() - start\n",
        "print(\"{} s to get output\".format(duration))\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\t\t\t\t\t\t\tpredictions.argmax(axis=1), target_names=labelNames, digits=5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl--NFYP5hTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# determine the number of epochs and then construct the plot title\n",
        "N = np.arange(0, NUM_EPOCHS)\n",
        "title = \"Training Loss and Accuracy on CIFAR-10)\"\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(title)\n",
        "plt.xlabel(\"Epoch №\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.savefig('/content/drive/My Drive/Colab/checkpoint_new.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9HnUZhoAQnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}